{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Trees\n",
    "\n",
    "## Do two questions in total: \"Q1+Q2\" or \"Q1+Q3\"\n",
    "\n",
    "`! git clone https://github.com/ds3001f25/linear_models_assignment.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Please answer the following questions in your own words.\n",
    "1. Why is the Gini a good loss function for categorical target variables? \n",
    "2. Why do trees tend to overfit, and how can this tendency be constrained? \n",
    "3. True or false, and explain: Trees only really perform well in situations with lots of categorical variables as features/covariates. \n",
    "4. Why don't most versions of classification/regression tree concept allow for more than two branches after a split?\n",
    "5. What are some heuristic ways you can examine a tree and decide whether it is probably over- or under-fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Gini Impurity index is a good loss function for categoricals because the goal of it is to find the most pure by calculation of categorical impurity within a group. That ends up being a really good loss function because for classification tasks we want to have the best means of identifying/discriminating what the best category would be, so calculating impurity and splitting on the most pure features would lead to this since we would be aiming for the best purity in splits (and thus the best identification/discrimination).\n",
    "\n",
    "2. Trees tend to overfit when you do not limit their depth. This is because ultimately a tree that has no limitation can just split as many times as it needs to until it eventually \"learns\" your entire training dataset (which is the definition of overfitting since it can no longer generalize well). This tendency can again be trained by limiting the depth of the tree and thereby forcing it to no remember your whole train set.\n",
    "\n",
    "3. False - they can work well in situations without lots of categoricals as features/covariates, and they can work well with numericals too.\n",
    "\n",
    "4. So I'm gonna answer this question based on what I remember from my CS machine learning class. First of all, as far as I know you can get the same results from a binary split only tree as a multi-split tree (because all you'd do is binary split on one split, and then you'd binary split on the other thing you wanted to split on - mimicking a multi-split). Another thing is that although it might make more sense for categoricals (i.e. let's say for instance, we wanted to split and make nodes on whether the weather is clear, sunny, or windy), for numericals it becomes a little bit more convoluted to have to calculate the optimal \"splits\" (i.e. you'd have to calculate the entropy multiple times to find the optimal). Lastly, and now that I think about it, you can't actually compute the Gini index on non-binary splits (which isn't necessarily an issue since we have entropy) but the point is this might be another reason why.\n",
    "\n",
    "5. One of the best heuristic ways to examine a tree (just look at it) and determine whether it is overfitting in particular is, does it have a ton of depth/leafs/splits? If it does, it might be splitting too much (and thus, \"memorizing\" the data by splitting on every pattern rather than just getting the best optimized splits). One way to tell if a tree is underfitting is the opposite - if there is not too much depth/leafs/splits it might indicate that the tree did not capture the necessary granularity necessary to capture the optimal classification split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** This is a case study about classification and regression trees.\n",
    "\n",
    "1. Load the `Breast Cancer METABRIC.csv` dataset. How many observations and variables does it contain? Print out the first few rows of data.\n",
    "\n",
    "2.  We'll use a consistent set of feature/explanatory variables. For numeric variables, we'll include `Tumor Size`, `Lymph nodes examined positive`, `Age at Diagnosis`. For categorical variables, we'll include `Tumor Stage`, `Chemotherapy`, and `Cancer Type Detailed`. One-hot-encode the categorical variables and concatenate them with the numeric variables into a feature/covariate matrix, $X$.\n",
    "\n",
    "3. Let's predict `Overall Survival Status` given the features/covariates $X$. There are 528 missing values, unfortunately: Either drop those rows from your data or add them as a category to predict. Constrain the minimum samples per leaf to 10. Print a dendrogram of the tree. Print a confusion matrix of the algorithm's performance. What is the accuracy? \n",
    "\n",
    "4. For your model in part three, compute three statistics:\n",
    "    - The **true positive rate** or **sensitivity**:\n",
    "        $$\n",
    "        TPR = \\dfrac{TP}{TP+FN}\n",
    "        $$\n",
    "    - The **true negative rate** or **specificity**:\n",
    "        $$\n",
    "        TNR = \\dfrac{TN}{TN+FP}\n",
    "        $$\n",
    "    Does your model tend to perform better with respect to one of these metrics?\n",
    "\n",
    "5. Let's predict `Overall Survival (Months)` given the features/covariates $X$. Use the train/test split to pick the optimal `min_samples_leaf` value that gives the highest $R^2$ on the test set (it's about 110). What is the $R^2$? Plot the test values against the predicted values. How do you feel about this model for clinical purposes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** This is a case study about trees using bond rating data. This is a dataset about bond ratings for different companies, alongside a bunch of business statistics and other data. Companies often have multiple reviews at different dates. We want to predict the bond rating (AAA, AA, A, BBB, BB, B, ..., C, D). Do business fundamentals predict the company's rating?\n",
    "\n",
    "1. Load the `./data/corporate_ratings.csv` dataset. How many observations and variables does it contain? Print out the first few rows of data.\n",
    "\n",
    "2.  Plot a histogram of the `ratings` variable. It turns out that the gradations of AAA/AA/A and BBB/BB/B and so on make it hard to get good results with trees. Collapse all AAA/AA/A ratings into just A, and similarly for B and C.\n",
    "\n",
    "3. Use all of the variables **except** Rating, Date, Name, Symbol, and Rating Agency Name. To include Sector, make a dummy/one-hot-encoded representation and include it in your features/covariates. Collect the relevant variables into a data matrix $X$. \n",
    "\n",
    "4. Do a train/test split of the data and use a decision tree classifier to predict the bond rating. Including a min_samples_leaf constraint can raise the accuracy and speed up computation time. Print a confusion matrix and the accuracy of your model. How well do you predict the different bond ratings?\n",
    "\n",
    "5. If you include the rating agency as a feature/covariate/predictor variable, do the results change? How do you interpret this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
